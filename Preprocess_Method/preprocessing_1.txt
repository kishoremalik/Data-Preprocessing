## preprocessing 1
========================

1) missing value 
data.isnull().sum()

2) out lier removing with quantile method
-------------------------------------------
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1

data_final = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]

3) numerical variable selecting

numerical_cols=data_final.select_dtypes(include=['int64','int32','float64','float32'])

4) Data sampling 

from sklearn.model_selection import train_test_split

target=data_final['rv2']
feature=data_final.drop(['rv1','rv2'],axis=1)


5) managing imbalanced data with SMOTE
------------------------------------------
from imblearn.over_sampling import SMOTE
target_x=data_final['Outcome']
train_data=data_final.drop('Outcome',axis=1)

smot=SMOTE(random_state=0, n_jobs=8)
x,y=smot.fit_resample(train_data,target_x)

6) learning curve
------------------------------------------------------------------------------------
from sklearn.model_selection import learning_curve
model2=LogisticRegression()
sizes, training_scores, testing_scores = learning_curve(LogisticRegression(), new_df, label, cv=10, scoring='accuracy')

import matplotlib.pyplot as plt
# Mean and Standard Deviation of training scores
mean_training = np.mean(training_scores, axis=1)
Standard_Deviation_training = np.std(training_scores, axis=1)
  
# Mean and Standard Deviation of testing scores
mean_testing = np.mean(testing_scores, axis=1)
Standard_Deviation_testing = np.std(testing_scores, axis=1)
  
# dotted blue line is for training scores and green line is for cross-validation score
plt.plot(sizes, mean_training, '--', color="b",  label="Training score")
plt.plot(sizes, mean_testing, color="g", label="Cross-validation score")
  
# Drawing plot
plt.title("LEARNING CURVE FOR KNN Classifier")
plt.xlabel("Training Set Size"), plt.ylabel("Accuracy Score"), plt.legend(loc="best")
plt.tight_layout()
plt.show()

##  Random forest Hyper tuning
-------------------------------------------------------------------------------------------------------
from sklearn.model_selection import GridSearchCV
n_estimators = [100, 300, 500, 800, 1200]
max_depth = [5, 8, 15, 25, 30]
min_samples_split = [2, 5, 10, 15, 100]
min_samples_leaf = [1, 2, 5, 10] 

forest= RandomForestClassifier()
hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  
              min_samples_split = min_samples_split, 
             min_samples_leaf = min_samples_leaf)

gridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, 
                      n_jobs = -1)
bestF = gridF.fit(x_train, y_train)